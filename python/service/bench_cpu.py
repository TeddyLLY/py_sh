import time
from datetime import datetime
from multiprocessing import Pool, cpu_count
from flask import jsonify
from python.util.common import get_logger
import os

logger = get_logger()



def compute_partial(start, end):
    pid = os.getpid()

    logger.info(f"[PID {pid}] è¨ˆç®—å€æ®µé–‹å§‹: {start} ~ {end - 1}")

    segment_start_time = time.time()

    partial_total = 0
    for i in range(start, end):
        partial_total += (i % 5) * (i % 3)

    segment_end_time = time.time()
    elapsed = round(segment_end_time - segment_start_time, 4)

    logger.info(
        f"[PID {pid}] å€æ®µå®Œæˆï¼Œçµæœ: {partial_total}ï¼Œè€—æ™‚: {elapsed} ç§’"
    )

    return partial_total

def bench_cpu_result(count=10000000):
    logger.info(f"ğŸ§ª é–‹å§‹ CPU æ¸¬è©¦ä»»å‹™ï¼Œç¸½è¨ˆç®—é‡ï¼š{count}")

    start_time = datetime.utcnow().isoformat() + "Z"
    start = time.time()

    num_processes = cpu_count()
    logger.info(f"ğŸ§  ç³»çµ±åµæ¸¬å¯ç”¨ CPU æ ¸å¿ƒæ•¸ï¼š{num_processes}")

    chunk_size = count // num_processes
    ranges = [(i * chunk_size, (i + 1) * chunk_size) for i in range(num_processes)]

    if count % num_processes != 0:
        ranges[-1] = (ranges[-1][0], count)
    logger.debug(f"ğŸ“Š åˆ†é…è¨ˆç®—å€æ®µç¯„åœï¼š{ranges}")

    logger.info("ğŸš€ å•Ÿå‹• multiprocessing è¨ˆç®—")
    with Pool(processes=num_processes) as pool:
        results = pool.starmap(compute_partial, ranges)

    total = sum(results)
    logger.info(f"âœ… è¨ˆç®—å®Œæˆï¼Œçµæœ total={total}")

    end = time.time()
    end_time = datetime.utcnow().isoformat() + "Z"
    elapsed = round(end - start, 4)

    logger.info(f"âŒ› CPU è¨ˆç®—è€—æ™‚ï¼š{elapsed} ç§’")

    return jsonify({
        "total": total,
        "time_sec": elapsed,
        "cpu_count": num_processes,
        "start_time": start_time,
        "end_time": end_time
    })